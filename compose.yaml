version: "3.8"

services:
  nifi:
    build:
      context: .
      dockerfile: docker-NiFi/Dockerfile
    image: nifi-with-hdfs:1.0
    ports:
      - "8443:8443"
    environment:
      - NIFI_SECURITY_USER_OIDC_ENABLED=false
      - NIFI_WEB_HTTPS_PORT=8443
      - SINGLE_USER_CREDENTIALS_USERNAME=admin
      - SINGLE_USER_CREDENTIALS_PASSWORD=passwordpass
    volumes:
      - nifi_data:/opt/nifi/nifi-current/data
      - ./docker-NiFi/hadoop-conf:/opt/hadoop-conf   # montiamo config Hadoop dentro NiFi
      - ./scripts:/app                               # Monta gli script Spark
    networks:
      - hadoop_network

  hadoop-master:
    build: ./docker-hadoop
    image: hadoop:3.3.6
    container_name: master
    hostname: master
    networks:
      - hadoop_network
    ports:
      - "9870:9870"    # NameNode Web UI
      - "54310:54310"  # Porta RPC NameNode
    stdin_open: true
    tty: true

  hadoop-slave1:
    build: ./docker-hadoop
    image: hadoop:3.3.6
    container_name: slave1
    hostname: slave1
    depends_on:
      - hadoop-master
    networks:
      - hadoop_network
    ports:
      - "9864:9864"  # DataNode
    stdin_open: true
    tty: true

  hadoop-slave2:
    build: ./docker-hadoop
    image: hadoop:3.3.6
    container_name: slave2
    hostname: slave1
    depends_on:
      - hadoop-master
    networks:
      - hadoop_network
    ports:
      - "9863:9864"  # DataNode
    stdin_open: true
    tty: true

  hadoop-slave3:
    build: ./docker-hadoop
    image: hadoop:3.3.6
    container_name: slave3
    hostname: slave1
    depends_on:
      - hadoop-master
    networks:
      - hadoop_network
    ports:
      - "9862:9864"  # DataNode
    stdin_open: true
    tty: true

  spark-master:
    build:
      context: .
      dockerfile: docker-spark/Dockerfile
    image: spark-ralisin:1.0
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - HOME=/home
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - ./scripts:/app
    networks:
      - hadoop_network


  spark-worker-1:
    build:
      context: .
      dockerfile: docker-spark/Dockerfile
    image: spark-ralisin:1.0
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - HOME=/home
    depends_on:
      - spark-master
    volumes:
      - ./scripts:/app
    networks:
      - hadoop_network

  spark-worker-2:
    build:
      context: .
      dockerfile: docker-spark/Dockerfile
    image: spark-ralisin:1.0
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - HOME=/home
    depends_on:
      - spark-master
    volumes:
      - ./scripts:/app
    networks:
      - hadoop_network

  influxdb:
    image: influxdb:2.7
    container_name: influxdb
    ports:
      - "8086:8086"
    volumes:
      - influxdb_data:/var/lib/influxdb
    networks:
      - hadoop_network
      - grafana_network
    environment:
      - DOCKER_INFLUXDB_INIT_MODE=setup
      - DOCKER_INFLUXDB_INIT_USERNAME=admin
      - DOCKER_INFLUXDB_INIT_PASSWORD=admin123
      - DOCKER_INFLUXDB_INIT_ORG=ralisin
      - DOCKER_INFLUXDB_INIT_BUCKET=init_bucket
      - DOCKER_INFLUXDB_INIT_ADMIN_TOKEN=my-super-secret-token

  grafana:
    image: grafana/grafana
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
    depends_on:
      - influxdb
    networks:
      - grafana_network

volumes:
  nifi_data:
  influxdb_data:
  grafana_data:

networks:
  hadoop_network:
    driver: bridge
  grafana_network:
    driver: bridge